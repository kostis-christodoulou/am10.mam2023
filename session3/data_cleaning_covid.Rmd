---
title: "Session 3 (Data Visualization): Cleaning Covid Data"
author: "Your name goes here"
date: "`r Sys.Date()`"
output: 
    html_document:
      number_sections: true
      highlight: zenburn
      theme: flatly
      toc: yes
      toc_depth: 2
      toc_float:
        collapsed: false
      fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(vroom)
library(janitor)
library(skimr)
library(vroom)
library(mice) 
library(VIM)

```

# Exploring Covid data

## Data

Covid 19 is a respiratory disease caused by a new virus that has been declared a pandemic by the World Health Organization. The purpose of this exercise is to investigate how the virus spread across the globe over time and to create a dashboard that allows a user to make their own investigation and reach their own conclusions. Not only do we want to see the various wavs of cases/hospitalisations/deaths, but we can also examine the race against covid with vaccinations around the world.

Any visualization exercise starts with good quality data. Arguably, the quality of data on the number of cases and deaths due to Covid 19 is not great because all countries have had limited testing capacity (so a lot of cases have been left undiagnosed /undetected) and some may actively distort numbers for political reasons. Any data mining project has such data limitations and bearing these in mind is important. One of the most reliable sources of up to date data is the *Our world in Data* website

```{r, get_data}
url <- "https://covid.ourworldindata.org/data/owid-covid-data.csv"
covid_data <- vroom(url) %>% 
  mutate(perc = round(100 * people_fully_vaccinated / population, 2))

```


Where to start analyzing a raw data set.

-   Transform data into technically correct data

1.  Each column has the same type of data that is consistent with what data in that column represents.
2.  Identify missing data.

-   Transform technically correct data into consistent data

1.  Handle missing values.
2.  Handle special values (eg., NA, N/A, inf..)
3.  Check for errors
4.  Check for outliers (eg., age=150)
5.  Check consistency between columns (eg., age=5, marital status=married)


# Inspect the data


```{r load_data}



#Now we can use other summary functions to have a more general idea about what is in the data
glimpse(covid_data)

# always take a look at what's in the data first; skir::skim() is perhaps
# the most important tool you have. It will take a while to run
skimr::skim(covid_data)


```

Make sure you understand what each variable represents. What units is each variable in (see above for column explanations)? Is data type of each column consistent with what you would expect?

Recall that our eventual goal is to estimate future usage of BBC iPlayer users. We will clean and process the data to prepare it for analysis.

# Technically correct data

In this step

1. Remove empty columns and rows and duplicates.
1. Check how many values are missing.
1. Correct data types, if necessary
1. Check min, max, and distribution of numerical values.
1. Names and distributions (counts) of categorical values.
1. Fix any additional irregularities.